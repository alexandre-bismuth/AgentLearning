{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ad20b64",
   "metadata": {},
   "source": [
    "# Agent Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a25f3ef-24ac-4345-b798-a4adf8331337",
   "metadata": {},
   "source": [
    "**This code is not yet functional. I am currently in the process of implementing it. I will provide with updates as soon as I can.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e32146",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b87ba5c",
   "metadata": {},
   "source": [
    "We start by importing the required libaries. Uncomment & run the next cell in case some of the libraries aren't installed in your environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca144d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy matplotlib\n",
    "# !pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77b624f2-6a36-4614-950e-dffd0147b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have to run this because I have a kernel problem but you shouldn't have to run it\n",
    "run = True\n",
    "if run:\n",
    "    import sys\n",
    "    sys.path.append('/opt/homebrew/lib/python3.11/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5789207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Useful for computations and probabilistic distributions\n",
    "import matplotlib.pyplot as plt # Plotting Graph for Simulation results\n",
    "from collections import defaultdict # Specific kind of dictionary that avoids KeyErrors\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddf852f",
   "metadata": {},
   "source": [
    "## Defining the Agent class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eebab1",
   "metadata": {},
   "source": [
    "We define a type of agent with the following attributes: \n",
    "- Agent name : Name of the agent\n",
    "- Service probability : If at the counter, shows the probability that this agent will be serviced.\n",
    "- Reward : Reward the counter gets for servicing this agent\n",
    "- Time in queue : Time for which the agent has been in the queue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f684bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, agent_name):\n",
    "        self.agent_name = agent_name\n",
    "        self.service_prob = agents[agent_name][0]\n",
    "        self.reward = agents[agent_name][1]\n",
    "        self.penalty = agents[agent_name][2]\n",
    "        self.time_in_queue = 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Agent {self.agent_name} ({self.time_in_queue})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d38814",
   "metadata": {},
   "source": [
    "## Defining the Queue mechanism "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d01dd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueueSimulation:\n",
    "    def __init__(self, agents, arrival_prob, strategy, init, alpha, discount):\n",
    "        # Define the agents, their distribution, and the strategy\n",
    "        self.agents = agents\n",
    "        self.agent_distribution = np.array([agent[3] for agent in agents.values()])\n",
    "        self.agent_distribution = self.agent_distribution / np.sum(self.agent_distribution)\n",
    "        self.strategy = strategy\n",
    "        \n",
    "        # Define the hyper-parameters of the simulation\n",
    "        self.arrival_prob, self.init, self.alpha, self.discount = arrival_prob, init, alpha, discount\n",
    "        self.fixed_discount = self.discount # Optimization trick to avoid recomputing power at each step\n",
    "        \n",
    "        # Define the Queue\n",
    "        self.queue = [self.create_agent() for _ in range(init)] if type(init) == int else init\n",
    "        self.current_agent = None if len(self.queue) == 0 else self.queue[0]\n",
    "        \n",
    "        # Variables that are used to analyze our code\n",
    "        self.time, self.busy, self.total_reward, self.discounted_reward = 0, 0, 0, 0\n",
    "        \n",
    "        # Tracking the history for graphs\n",
    "        self.serviced_agents = defaultdict(int)\n",
    "        self.reward_history, self.queue_length_history = [], []\n",
    "\n",
    "    def create_agent(self):\n",
    "        agent_name = np.random.choice(list(self.agents.keys()), p=self.agent_distribution)\n",
    "        return Agent(agent_name)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        This function is the core of the simulation. It defines what happens at every step. The order in which \n",
    "        the computations are made is important so we have to be careful. In our case, we proceed as follows:\n",
    "        \n",
    "        1. Determine whether an agent is joining the queue\n",
    "        2. Detemine if the agent in service is going to complete service or not and potentially add reward \n",
    "        3. Apply the given strategy to determine what happens next\n",
    "        4. Add penalty to the reward\n",
    "        5. Add the information related to the step to the simulation history\n",
    "        \n",
    "        Please feel free to ask for any changes in the implementation\n",
    "        \"\"\"\n",
    "        self.time += 1 # Update time\n",
    "        \n",
    "        # Add new agent to the queue based on arrival probability\n",
    "        if np.random.rand() < self.arrival_prob:\n",
    "            new_agent = self.create_agent()\n",
    "            self.queue.append(new_agent)\n",
    "\n",
    "        # Check if the current agent is done being served\n",
    "        if self.current_agent:\n",
    "            if np.random.rand() < self.current_agent.service_prob:\n",
    "                self.total_reward += self.current_agent.reward\n",
    "                self.discounted_reward += self.current_agent.reward*self.discount\n",
    "                self.serviced_agents[self.current_agent.agent_name] += 1\n",
    "                self.current_agent = None\n",
    "                \n",
    "        # Apply the strategy to select the next agent\n",
    "        if self.queue:\n",
    "            queue_index = self.strategy(self.queue,self.current_agent)\n",
    "            self.swap_agents(queue_index)\n",
    "        \n",
    "        length = len(self.queue)\n",
    "        \n",
    "        # Apply penalty on reward\n",
    "        for agent in self.queue: \n",
    "            agent.time_in_queue += 1\n",
    "            self.total_reward -= agent.penalty\n",
    "            self.discounted_reward -= agent.penalty*self.discount\n",
    "        self.total_reward -= regularization(self.alpha,length)\n",
    "        self.discounted_reward -= regularization(self.alpha,length)*self.discount\n",
    "        \n",
    "        # Add the step's information to the simulation history\n",
    "        self.reward_history.append(self.total_reward)\n",
    "        self.queue_length_history.append(length+bool(self.current_agent))\n",
    "        self.busy += bool(self.current_agent)\n",
    "        \n",
    "        self.discount *= self.discount # Update Discount\n",
    "\n",
    "    def swap_agents(self, queue_index):\n",
    "        if 0 <= queue_index < len(self.queue):\n",
    "            if self.current_agent:\n",
    "                self.queue[queue_index], self.current_agent = (self.current_agent,self.queue[queue_index])\n",
    "            else:\n",
    "                self.current_agent = self.queue.pop(queue_index)\n",
    "\n",
    "    def run_simulation(self, steps=1000):\n",
    "        for _ in range(steps):\n",
    "            self.step()\n",
    "\n",
    "    def get_results(self):\n",
    "        # Plot total reward over time\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(self.reward_history)\n",
    "        plt.title('Total Reward over Time')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Total Reward')\n",
    "\n",
    "        # Plot queue length over time\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(self.queue_length_history)\n",
    "        plt.title('Queue Length over Time')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Queue Length')\n",
    "\n",
    "        # Plot distribution of serviced agents by type\n",
    "        plt.subplot(1, 3, 3)\n",
    "        agent_types, counts = zip(*sorted(self.serviced_agents.items()))\n",
    "        plt.bar(agent_types, counts)\n",
    "        plt.title('Distribution of Serviced Agents by Type')\n",
    "        plt.xlabel('Agent Type')\n",
    "        plt.ylabel('Count')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        self.min_queue_length = np.min(self.queue_length_history)\n",
    "        self.max_queue_length = np.max(self.queue_length_history)\n",
    "        self.mean_queue_length = np.mean(self.queue_length_history)\n",
    "        self.normalized_distribution = {agent_type: round(count / sum(self.serviced_agents.values()), 3) for agent_type, count in sorted(self.serviced_agents.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "        \n",
    "        print(f\"\\nTotal reward: {self.total_reward}\")\n",
    "        print(f\"Discounted reward: {self.discounted_reward}\")\n",
    "        print(f\"Average reward per step: {self.total_reward/self.time}\")\n",
    "        print(f\"Min & Max Queue Length: {self.min_queue_length} & {self.max_queue_length}\")\n",
    "        print(f\"Mean Queue Length: {self.mean_queue_length}\")\n",
    "        print(f\"Normalized Distribution of Serviced Agents: {self.normalized_distribution}\")\n",
    "        print(f\"Time spent busy: {self.busy/self.time}\")\n",
    "        print(f\"--------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b94968d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average service probability\n",
    "def service():\n",
    "    service_prob = np.array([agent[0] for agent in agents.values()])\n",
    "    weights = np.array([agent[3] for agent in agents.values()])\n",
    "    distribution = weights / np.sum(weights)\n",
    "    x = np.dot(service_prob, distribution.T)\n",
    "    return x,1/x\n",
    "\n",
    "# Compute expect reward per agent\n",
    "def average():\n",
    "    probabilities = np.array([agent[0] for agent in agents.values()])\n",
    "    rewards = np.array([agent[1] for agent in agents.values()])\n",
    "    penalties = np.array([agent[2] for agent in agents.values()])\n",
    "    average_values = {agent: (rewards[i] + penalties[i]) / probabilities[i] for i, agent in enumerate(agents.keys())}\n",
    "    return average_values\n",
    "\n",
    "# Create the initial list\n",
    "def create(l, shuffle=False):\n",
    "    queue = []\n",
    "    for name, quantity in l:\n",
    "        queue += [Agent(name)] * quantity\n",
    "    if shuffle:\n",
    "        np.random.shuffle(queue)\n",
    "    return queue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc4fb4d",
   "metadata": {},
   "source": [
    "## Choosing base parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89df262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Agents\n",
    "agents = {\n",
    "    ### Agent name : [service_probability, reward, penalty, distrib_weight]\n",
    "    # service_probability: float in [0,1]\n",
    "    # reward, penalty, distrib_weight: float (unconstrained)\n",
    "    \"Alex\" : [0.7, 1, 0.1, 6], \n",
    "    \"Ben\" : [0.5, 20, 0.4, 3],\n",
    "    \"Cameron\" : [0.3, 30, 0.2, 4],\n",
    "    \"Dennis\" : [0.4, 20, 0.01, 2],\n",
    "    \"Eric\": [0.8, 4, 0.2, 3],\n",
    "    \"Fabien\": [0.5, 50, 0.4, 1],\n",
    "}\n",
    "\n",
    "# Probabilty that a new agent joins the queue\n",
    "arrival_prob = 0.5\n",
    "\n",
    "# Number of agents in the queue at t = 0\n",
    "# We either choose a random weighted queue or a specific queue\n",
    "start = 0\n",
    "queue = create([(\"Fabien\", 20), (\"Alex\", 10)], True) # [(name, quantity),(name, quantity),...], shuffle=True/False \n",
    "init = start # Choose a mode here\n",
    "\n",
    "# Choose a discount factor for discounted reward computation\n",
    "discount = 0.99\n",
    "\n",
    "# Regularization parameter and function\n",
    "alpha = 0.00\n",
    "def regularization(alpha,length):\n",
    "    return alpha*length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8535ec88",
   "metadata": {},
   "source": [
    "## Running the Q-Learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f46c44a",
   "metadata": {},
   "source": [
    "Execute the main code automatically by choosing one of the strategies defined above in the 'Define Strategies' section and letting the computer play by itself. This function outputs three graphs. \n",
    "- Graph 1: Total reward from time $t_0$ to time $t_{1000}$.\n",
    "- Graph 2: Length of the Queue from time $t_0$ to time $t_{1000}$.\n",
    "- Graph 3: Distribution of serviced Agents by type from $t_0$ to time $t_{1000}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5c9ee49",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'strategy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m simulation \u001b[38;5;241m=\u001b[39m QueueSimulation(\n\u001b[1;32m      2\u001b[0m     agents,\n\u001b[1;32m      3\u001b[0m     arrival_prob,\n\u001b[0;32m----> 4\u001b[0m     strategy,\n\u001b[1;32m      5\u001b[0m     init,\n\u001b[1;32m      6\u001b[0m     alpha,\n\u001b[1;32m      7\u001b[0m     discount,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutomatic simulation with strategy \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected service probability & time across agents: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mservice()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'strategy' is not defined"
     ]
    }
   ],
   "source": [
    "simulation = QueueSimulation(\n",
    "    agents,\n",
    "    arrival_prob,\n",
    "    strategy,\n",
    "    init,\n",
    "    alpha,\n",
    "    discount,\n",
    ")\n",
    "\n",
    "print(f\"Automatic simulation with strategy '{strategy.__name__}'\")\n",
    "print(f\"Expected service probability & time across agents: {service()}\")\n",
    "print(f\"Expected reward + weighted penalty for each agent: {average()}\")\n",
    "simulation.run_simulation(steps=10000)\n",
    "simulation.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9afe3e4",
   "metadata": {},
   "source": [
    "# Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5db5795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cda7616",
   "metadata": {},
   "source": [
    "In case something needs to be changed, please feel free to write something in the cell below and email me about it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
